# download_model.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_PATH = "Qwen/Qwen2.5-7B-Instruct"
SAVE_PATH = "./models/Qwen2.5-7B-Instruct"  # æœ¬åœ°ä¿å­˜è·¯å¾„


def download_model():
    """ä¸‹è½½æ¨¡å‹å’Œtokenizeråˆ°æœ¬åœ°"""
    print(f"å¼€å§‹ä¸‹è½½æ¨¡å‹: {MODEL_PATH}")
    print("æ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå–å†³äºç½‘ç»œé€Ÿåº¦å’Œæ¨¡å‹å¤§å°...")

    try:
        # ä¸‹è½½tokenizer
        print("æ­£åœ¨ä¸‹è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        tokenizer.save_pretrained(SAVE_PATH)
        print(f"âœ… Tokenizerå·²ä¿å­˜åˆ°: {SAVE_PATH}")

        # ä¸‹è½½æ¨¡å‹
        print("æ­£åœ¨ä¸‹è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        model.save_pretrained(SAVE_PATH)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {SAVE_PATH}")

        print("ğŸ‰ æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
        print(f"ä½ å¯ä»¥ä¿®æ”¹è®­ç»ƒä»£ç ä¸­çš„ MODEL_PATH ä¸º: '{SAVE_PATH}'")

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

    return True


if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…transformers
    try:
        import transformers
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£…transformers: pip install transformers")
        exit(1)

    # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…torch
    try:
        import torch
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£…torch: pip install torch")
        exit(1)

    download_model()